{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5b2/RHmgFYI1l84XbjAab",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinwooChoi/ESAA-OB/blob/main/11_14_ESAA_OB_%ED%95%84%EC%82%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "범위: <딥러닝 파이토치 교과서> 4장 p.138-163"
      ],
      "metadata": {
        "id": "NVJ9zXImWY4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4장 딥러닝 시작\n",
        "\n",
        "### 4.1 인공 신경망의 한계와 딥러닝 출현\n",
        "- **퍼셉트론(Perceptron)**: 입력 신호를 받아 하나의 출력(1 또는 0)을 전달하는 가장 기본적인 인공 신경망.  \n",
        "- **AND 게이트**: 모든 입력이 1일 때만 출력이 1.  \n",
        "- **OR 게이트**: 입력 중 하나라도 1이면 출력이 1.  \n",
        "- **XOR 게이트**: 두 입력이 다를 때만 출력이 1. → **단층 퍼셉트론으로는 학습 불가능.**  \n",
        "- **다층 퍼셉트론(MLP)**: 은닉층을 추가하여 비선형 문제(XOR 등) 해결 가능.  \n",
        "- **딥러닝(Deep Learning)**: 은닉층이 여러 개인 신경망(심층 신경망, DNN).\n",
        "\n",
        "\n",
        "\n",
        "### 4.2 딥러닝 구조\n",
        "- **입력층(Input layer)**: 데이터를 받아들이는 층  \n",
        "- **은닉층(Hidden layer)**: 입력값에 가중치를 적용 → 활성화 함수에 전달  \n",
        "- **출력층(Output layer)**: 최종 결과를 출력  \n",
        "- **가중치(Weight)**: 입력의 중요도를 조절하는 값  \n",
        "- **바이어스(Bias)**: 출력을 조정하는 상수  \n",
        "- **가중합(Weighted sum)**: 입력×가중치의 합  \n",
        "- **활성화 함수(Activation function)**: 비선형성을 주어 신호를 변환  \n",
        "- **손실 함수(Loss function)**: 예측값과 실제값의 차이를 측정\n",
        "\n",
        "\n",
        "### 활성화 함수\n",
        "1. **시그모이드(Sigmoid)**  \n",
        "   - 출력: 0~1  \n",
        "   - 식: f(x) = 1 / (1 + e^(-x))  \n",
        "   - 단점: 기울기 소멸 문제 발생  \n",
        "\n",
        "2. **하이퍼볼릭 탄젠트(Tanh)**  \n",
        "   - 출력: -1~1  \n",
        "   - 평균 0으로 변환하지만 여전히 기울기 소멸 존재  \n",
        "\n",
        "3. **렐루(ReLU)**  \n",
        "   - x>0 → x, x≤0 → 0  \n",
        "   - 장점: 계산 빠르고 기울기 소멸 적음  \n",
        "   - 단점: 음수 입력 시 학습 불가  \n",
        "\n",
        "4. **리키 렐루(Leaky ReLU)**  \n",
        "   - x≤0일 때도 작은 기울기(0.001x 등) 부여  \n",
        "   - 렐루의 단점을 보완  \n",
        "\n",
        "5. **소프트맥스(Softmax)**  \n",
        "   - 출력값을 0~1 사이로 정규화, 전체 합 = 1  \n",
        "   - 분류 문제의 출력층에서 사용  \n",
        "\n",
        "\n",
        "\n",
        "### 손실 함수\n",
        "1. **평균 제곱 오차(MSE)**  \n",
        "   - 회귀 문제에 사용  \n",
        "   - 식: MSE = (1/n) Σ(y_i - ŷ_i)²  \n",
        "\n",
        "2. **크로스 엔트로피 오차(CEE)**  \n",
        "   - 분류 문제에 사용 (특히 one-hot 인코딩 시)  \n",
        "   - 식: CrossEntropy = -Σ y_i log(ŷ_i)\n",
        "\n",
        "### 딥러닝 학습\n",
        "- 순전파(feedforward)\n",
        "- 입력 → 은닉층 → 출력층으로 계산 진행하여 예측값 얻는 과정.\n",
        "\n",
        "- 역전파(backpropagation)\n",
        "- 손실 함수의 오차를 출력층→은닉층→입력층으로 전파하며 가중치 업데이트.\n",
        "\n",
        "### 딥러닝의 문제점과 해결 방안\n",
        "1) 과적합(overfitting): 훈련 데이터에 과하게 맞춤 → 검증 성능 저하  \n",
        "   해결: 드롭아웃(Dropout), 규제(L2), 데이터 증가  \n",
        "2) 기울기 소멸: 깊은 신경망에서 시그모이드/tanh 사용할 때 발생  \n",
        "   해결: ReLU 계열 사용  \n",
        "3) 학습이 느려짐: 모든 데이터로 학습할 때(BGD)  \n",
        "   해결: SGD, 미니배치 SGD\n",
        "\n",
        "1. 드롭아웃(Dropout)\n",
        "- 학습 시 일부 노드를 임의로 비활성화하여 과적합 방지.\n",
        "\n",
        "2. 경사 하강법 종류\n",
        "- 배치 경사 하강법(BGD): 전체 데이터 사용, 정확·느림  \n",
        "- 확률적 경사 하강법(SGD): 1개 샘플 단위, 빠르나 불안정  \n",
        "- 미니배치 SGD: 여러 개로 나눠 평균 → 가장 많이 사용\n",
        "\n",
        "3. 옵티마이저 종류\n",
        "- Adagrad: 많이 변한 가중치는 학습률 감소  \n",
        "- Adadelta: Adagrad 개선, 학습률 자동 조정  \n",
        "- RMSProp: 지수 이동 평균 사용, G 값 폭주 방지  \n",
        "- Momentum: 이전 이동 방향 관성 적용  \n",
        "- NAG: Momentum + 예측된 위치 기반 기울기  \n",
        "- Adam: Momentum + RMSProp 결합 (최고로 많이 씀)\n",
        "\n",
        "### 딥러닝의 장점\n",
        "- 특징 추출 자동화(Feature extraction)\n",
        "- 복잡한 비선형 관계 학습 가능\n",
        "- 대규모 데이터에서 좋은 성능\n",
        "\n",
        "### 4.3 딥러닝 알고리즘 종류\n",
        "1) DNN(심층 신경망): 은닉층 여러 개.\n",
        "2) CNN: 이미지 처리에 특화(합성곱+풀링).\n",
        "3) RNN: 시계열·텍스트에 특화(순환 구조).\n",
        "4) RBM: 가시층-은닉층만 연결된 확률 모델.\n",
        "5) DBN: RBM을 여러 층 쌓아 구성한 심층 신뢰 신경망.\n",
        "\n",
        "### 알고리즘 특징 요약\n",
        "- DNN: 일반 구조, 비선형 분류 가능.\n",
        "- CNN: 이미지·객체 인식, 공간 정보 유지, 필터 공유.\n",
        "- RNN: 시간 순서 의존, LSTM·GRU로 개선됨.\n",
        "- RBM: 차원축소·추천·잠재요인 학습에 유용.\n",
        "- DBN: RBM을 층층이 쌓아 사전학습+미세조정.\n"
      ],
      "metadata": {
        "id": "wdGFCPJ1YPgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MAJx5hDtYPTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BKrjDQxlYPRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "-HtL6vWIYBZl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7fxfsSN6WXIH"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, n_feature, n_hidden, n_output):\n",
        "    super(Net, self).__init__()\n",
        "    self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
        "    self.relu = torch.nn.ReLu(inplace=True)\n",
        "    self.out = torch.nn.Linear(n_hidden, n_output)\n",
        "    self.softmax = torch.nn.Softmax(dim=n_output)\n",
        "  def forward(self, x):\n",
        "    x = self.hidden(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.out(x)\n",
        "    x = self.softmax(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "\n",
        "#loss_fn=torch.nn.MSELoss(reduction='sum')\n",
        "#y_pred=model(x)\n",
        "#loss=loss_fn(y_pred, y)"
      ],
      "metadata": {
        "id": "Vy-D1N4hXldv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss=nn.CrossEntropyLoss()\n",
        "#input=torch.randn(5, 6, requires_grad=True)\n",
        "#target=torch.empty(3, dtype=torch.long).random_(5)\n",
        "#output=loss(input, target)\n",
        "#output.backward()"
      ],
      "metadata": {
        "id": "PTdaTmuEX7Yb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DropoutModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DropoutModel, self).__init__()\n",
        "    self.layer1 = torch.nn.Linear(784, 1200)\n",
        "    self.dropout1 = torch.nn.Dropout(0.5)\n",
        "    self.layer2 = torch.nn.Linear(1200, 1200)\n",
        "    self.dropout2 = torch.nn.Dropout(0.5)\n",
        "    self.layer3 = torch.nn.Linear(1200, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer1(x))\n",
        "    x = self.dropout1(x)\n",
        "    x = F.relu(self.layer2(x))\n",
        "    x = self.dropout2(x)\n",
        "    return self.layer3(x)"
      ],
      "metadata": {
        "id": "1WrkLgLnX76I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#class CustomDataset(Dataset):\n",
        "#    def __init__(self):\n",
        "#        self.x_data=[[1,2,3], [4,5,6], [7,8,9]]\n",
        "#        self.y_data=[[12], [18], [11]]\n",
        "#        def __len__(self):\n",
        "#            return len(self.x_data)\n",
        "#        def __getitem__(self, idx):\n",
        "#            x=torch.FloatTensor(self.x_data[idx])\n",
        "#            y=torch.FloatTensor(self.y_data[idx])\n",
        "#            return x, y\n",
        "#dataset=CustomDataset()\n",
        "#dataloader=DataLoader(\n",
        "#    dataset,\n",
        "#    batch_size=2,\n",
        "#    shuffle=True,\n",
        "#)"
      ],
      "metadata": {
        "id": "yKqBb6IjYIYt"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}